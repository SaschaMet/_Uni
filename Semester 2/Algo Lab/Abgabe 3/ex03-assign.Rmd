---
title: "Übung 3"
lecture: "DSIA Algorithmik & Statistik, SS 2020"
author: "Sascha Metzger - 1910837830"
date: '2019-01-01 (updated: `r Sys.Date()`)'
output:
  html_document:
    df_print: paged
---

```{r options, include = FALSE}
knitr::opts_chunk$set(fig.align = "center")
```

Bitte um Beachtung der [Übungs-Policy](https://algo2-lab.netlify.com/%C3%BCbungs-policy.html) für genaue Anweisungen und einige Beurteilungsnotizen. Fehler bei der Einhaltung ergeben Punktabzug.

Angedachte Bearbeitungszeit: ca. 4 Std.

***
```{r setup, warning=FALSE, message=F}
library(dplyr)
library(caret)
library(mlbench)
library(ggplot2)
library(kableExtra)

library(randomForest)
library(gbm)
library(rpart)
library(rpart.plot)
```


# Aufgabe 1 (Computation Time)

**[8 points]** Für diese Übung werden wir Daten mittels Simulation erstellen und dann beurteilen, wie gut bestimmte Methoden funktionieren. Verwenden Sie den folgenden Code, um ein Trainings- und Testdatensatz zu erstellen.

```{r, message = FALSE, warning = FALSE}
set.seed(42)
sim_trn = mlbench.spirals(n = 2500, cycles = 1.5, sd = 0.125)
sim_trn = data.frame(sim_trn$x, class = as.factor(sim_trn$classes)) %>% tibble()
sim_tst = mlbench.spirals(n = 10000, cycles = 1.5, sd = 0.125)
sim_tst = data.frame(sim_tst$x, class = as.factor(sim_tst$classes)) %>% tibble()
```

Die Trainingsdaten sind unten dargestellt, wobei die Farben die Variable "Klasse" = Response sind.

```{r, fig.height = 5, fig.width = 5, echo = FALSE}
ggplot(sim_trn, aes(x=X1, y=X2, col=class))+geom_point()
```

Bevor Sie fortfahren, stelle einen Seed ein, der deiner StudentenID entspricht.

```{r}
uin = 1910897920
set.seed(uin)
```

Wir werden das Folgende verwenden, um eine 5-fache Kreuzvalidierung für die Verwendung mit `train()` von `caret` zu definieren.

```{r, message = FALSE, warning = FALSE}
library(caret)
cv_5 = trainControl(method = "cv", number = 5)

# tidymodels
library(tidymodels)
rec_cv_5 = vfold_cv(sim_trn, v=5)
```

Wir tunen nun zwei Modelle mit `train()`. Zuerst eine logistische Regression mit `glm`. (Dies ist eigentlich nicht "getunt", da es keine zu tunenden Parameter gibt, aber wir verwenden `train()`, um eine Kreuzvalidierung durchzuführen.) Zweitens tunen wir einen einzelnen Entscheidungsbaum mit `rpart`.

Wir speichern die Ergebnisse in `sim_glm_cv` bzw. `sim_tree_cv`, aber wir verpacken auch beide Funktionsaufrufe mit `system.time()`, um aufzuzeichnen, wie lange der Tuning-Prozess für jede Methode dauert.

```{r, message = FALSE, warning = FALSE}
glm_cv_time = system.time({
  sim_glm_cv  = train(
    class ~ .,
    data = sim_trn,
    trControl = cv_5,
    method = "glm")
})

tree_cv_time = system.time({
  sim_tree_cv = train(
    class ~ .,
    data = sim_trn,
    trControl = cv_5,
    method = "rpart")
})

sim_glm_cv_s <- predict(sim_glm_cv, sim_tst)
results <- confusionMatrix(data=sim_glm_cv_s, reference=sim_tst$class)
glmAccuracy <- results$overall["Accuracy"]
cat("GLM", glmAccuracy)

print("---")

sim_tree_cv_s <- predict(sim_tree_cv, sim_tst)
results <- confusionMatrix(data=sim_tree_cv_s, reference=sim_tst$class)
treeAccuracy <- results$overall["Accuracy"]
cat("TREE", treeAccuracy)
```

Wir sehen, dass beide Methoden durch Cross-Validierung in ähnlicher Zeit optimiert werden. Das "tree_cv_time" ist nur um ca. 100ms langsamer.

```{r}
glm_cv_time["elapsed"]
tree_cv_time["elapsed"]
```

```{r, message = FALSE, warning = FALSE, echio = FALSE}
library(rpart.plot)
rpart.plot(sim_tree_cv$finalModel)
```

Wiederholen Sie die obige Analyse mit einem Random Forest, zweimal. Verwenden Sie beim ersten Mal die 5-fache Kreuzvalidierung. Beim zweiten Mal optimieren wir das Modell mit OOB-Samples. Wir haben hier nur zwei Prädiktoren, also verwenden Sie für beide das folgende Tuning-Grid.

```{r}
rf_grid = expand.grid(mtry = c(1, 2))

rf_tidy = rand_forest(mtry=tune()) %>% 
  set_mode("classification") %>% 
  set_engine("randomForest")

rf_ctrl <- control_grid()

rf_cv_time = system.time({
  sim_rf_cv = train(
    class ~ .,
    data = sim_trn,
    trControl = cv_5,
    tuneGrid = rf_grid)
})


oob = trainControl(method = "oob")
rf_oob_time = system.time({
  sim_rf_oob = train(
    class ~ .,
    data = sim_trn,
    trControl = oob,
    tuneGrid = rf_grid)
})

```

```{r}
sim_rf_cv_s <- predict(sim_rf_cv, sim_tst)
results <- confusionMatrix(data=sim_rf_cv_s, reference=sim_tst$class)
cvAccuracy <- results$overall["Accuracy"]
cat("CV", cvAccuracy)

print("---")

sim_rf_oob_s <- predict(sim_rf_oob, sim_tst)
results <- confusionMatrix(data=sim_rf_oob_s, reference=sim_tst$class)
obbAccuracy <- results$overall["Accuracy"]
cat("OOB", obbAccuracy)
```

```{r}
rf_cv_time["elapsed"]
rf_oob_time["elapsed"]
```


Erstellen Sie eine Tabelle, in der die Ergebnisse dieser vier Modelle zusammengefasst sind. (Logistik mit CV, Baum mit CV, RF mit OOB, RF mit CV). Berichte:

- Gewählter Wert des Tuning-Parameters (falls zutreffend)
- Verstrichene Abstimmzeit
- Resampled (CV oder OOB) Genauigkeit
- Testgenauigkeit

```{r}
result_1 <- tribble(~model, ~parameter, ~training_time, ~resampled_accuracy, ~test_accuracy,
                    "Logistik mit CV", "none", as.double(glm_cv_time["elapsed"]), max(sim_glm_cv$results$Accuracy), as.double(glmAccuracy), 
                    "Baum mit CV", "single decision tree", as.double(tree_cv_time["elapsed"]), max(sim_tree_cv$results$Accuracy), as.double(treeAccuracy), 
                    "RF mit CV", "cross validation", as.double(rf_cv_time["elapsed"]), max(sim_rf_cv$results$Accuracy), as.double(cvAccuracy),  
                    "RF mit OBB", "mtry", as.double(rf_oob_time["elapsed"]), max(sim_rf_oob$results$Accuracy), as.double(obbAccuracy)) 
print(result_1)
```



# Aufgabe 2 (Predicting Baseball Salaries)

**[7 points]** Für diese Frage werden wir das `Gehalt` von `Hitters` vorhersagen. (`Hitters` ist auch der Name des Datensatzes.) Wir entfernen zuerst die fehlenden Daten:

```{r}
library(ISLR)
Hitters = na.omit(Hitters)
```

Nachdem du `uin` auf Ihre StudentID umgestellt hast, verwende den folgenden Code, um die Daten aufzuteilen.

```{r}
uin = 1910837830
set.seed(uin)
hit_split = initial_split(Hitters, p = 0.6, list = FALSE)
hit_trn = hit_split %>% training()
hit_tst = hit_split %>% testing()
```

Gehen wie folgt vor:

- Tunen Sie ein verstärktes Baummodell mit dem folgenden Tuning-Grid und der 5-fachen Kreuzvalidierung.

```{r}
gbm_grid = expand.grid(interaction.depth = c(1, 2),
                       n.trees = c(500, 1000, 1500),
                       shrinkage = c(0.001, 0.01, 0.1),
                       n.minobsinnode = 10)
```

- Tunen Sie einen Random Forest mit OOB Resampling und **allen** möglichen Werte von `mtry`. 

Erstellen Sie eine Tabelle, in der die Ergebnisse von drei Modellen zusammengefasst sind:

- Tuned boosted tree Model
- Tuned random forest Model
- Bagged tree Model

Für jedes, Berichte:

- Resampled RMSE
- Test RMSE

```{r}
#tuned boosted tree Model
tuned_boosted_tree = train(Salary ~ ., data = hit_trn,
                method = "gbm",
                trControl = cv_5,
                verbose = FALSE,
                tuneGrid = gbm_grid)

#Tuned Random Forest Model
rf_grid = rf_grid = expand.grid(mtry = 1:(ncol(hit_trn) - 1))
tuned_rf  = train(Salary ~ ., data = hit_trn,
                method = "rf",
                trControl = oob,
                tuneGrid = rf_grid)

#Bagged Tree
bagged_tree = train(Salary ~ ., data = hit_trn,
                method = "rf",
                trControl = oob,
                tuneGrid = data.frame(mtry = (ncol(hit_trn) - 1)))


#Calc RMSE
calc_rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

tbt_rmse_trn = calc_rmse(predicted = predict(tuned_boosted_tree, hit_trn),
                         actual = hit_trn$Salary)

rf_rmse_trn = calc_rmse(predicted = predict(tuned_rf, hit_trn),
                        actual = hit_trn$Salary)

bt_rmse_trn = calc_rmse(predicted = predict(bagged_tree, hit_trn),
                        actual = hit_trn$Salary)

tbt_rmse_tst = calc_rmse(predicted = predict(tuned_boosted_tree, hit_tst),
                         actual= hit_tst$Salary)

rf_rmse_tst = calc_rmse(predicted = predict(tuned_rf, hit_tst),
                        actual = hit_tst$Salary)

bt_rmse_tst = calc_rmse(predicted = predict(bagged_tree, hit_tst),
                        actual = hit_tst$Salary)

result_2 <- tribble(~model, ~train_rmse, ~test_rmse,
                    "Tuned boosted tree Model", tbt_rmse_trn, tbt_rmse_tst,
                    "Tuned random forest Model", rf_rmse_trn, rf_rmse_tst,
                    "Bagged tree Model", bt_rmse_tst, bt_rmse_tst)
result_2
```



# Aufgabe 3 (Transforming der Response)

**[5 points]** Dann fahren wir mit den Daten aus Übung 2 fort. Das Buch, ISL, schlägt vor, die Response "Salary", log-transformieren, bevor es in einen Random Forest passt. Ist das notwendig? Tunen Sie einen Random Forest neu, wie Sie es in Übung 2 getan haben, außer mit einer logarithmisch transformierten Response. Berichten Sie den Test RMSE sowohl für das nicht transformierte als auch für das transformierte Modell im Originalmaßstab der Antwortvariablen. 

```{r, echo = FALSE}
hit_trn_transformed <- hit_trn
hit_trn_transformed$Salary <- log(hit_trn_transformed$Salary)

hit_tst_transformed <- hit_tst
hit_tst_transformed$Salary <- log(hit_tst_transformed$Salary)

hist(hit_trn$Salary)
hist(hit_trn_transformed$Salary)
```
Die Variable zu transformieren ist nicht "notwendig", jedoch verbessert es die Aussagekraft des Trees. Decision Trees sind unempfindlich gegenüber der Skala der Variablen, aber da es eine kleine Anzahl hoher Gehälter gibt, verbessert die Log-Transformation der "Salary" Variable die Vorhersage, da der Tree (und seine Loss-Function) nicht von den großen Werten beeinflusst werden.

```{r rf transformed}
rf_grid = expand.grid(mtry = 1:(ncol(hit_trn_transformed) - 1))

transformed_rf  = train(Salary ~ ., data = hit_trn,
                        method = "rf",
                        trControl = oob,
                        tuneGrid = rf_grid)

transformed_rf_transformed  = train(Salary ~ ., data = hit_trn_transformed,
                        method = "rf",
                        trControl = oob,
                        tuneGrid = rf_grid)

#Calc RMSE
rmse_rf_hit_tst = calc_rmse(predicted = predict(transformed_rf, hit_tst),
                        actual = hit_tst$Salary)

rmse_rf_hit_tst_transformed = calc_rmse(predicted = predict(transformed_rf_transformed, hit_tst_transformed),
                         actual= hit_tst_transformed$Salary)


transformed_rf

```


```{r}
result_3 <- tribble(~model, ~test_rmse_base, ~test_rmse_log, 
                    "Log Transformed Random Forrest", rmse_rf_hit_tst, rmse_rf_hit_tst_transformed)
result_3
```


# Aufgabe 4 (Concept Checks)

**[1 point each]** Beantworte die folgenden Fragen auf der Grundlage deiner Ergebnisse aus den drei Übungen. 


**(a)** Vergleichen Sie die Zeit, die für die Abstimmung der einzelnen Modelle benötigt wird. Ist der Unterschied zwischen dem OOB- und dem CV-Ergebnis für den Random Forest ähnlich, wie Sie es erwartet hätten?
Das OOB-Ergebnis ist deutlich schneller als die CV. Einerseits weil die Cross-Validation mehrmals über den Datensatz iteriert und zweitens, weil beide Funktionen unterschiedliche Defaults setzen wie sie die Daten bearbeiten (https://stats.stackexchange.com/questions/198839/evaluate-random-forest-oob-vs-cv/199201)

**(b)** Vergleiche den getunten Wert von `mtry` für jeden der getunten Random Forests. Wählen sie das gleiche Modell?
Für beide RFs wurde jeweils 1 als Wert für mtry verwendet. Sie wählen also somit das gleiche Modell.

**(c)** Vergleichen Sie die Testgenauigkeit der vier betrachteten Verfahren. Erläutere diese Ergebnisse kurz.
Die Genaugikeiten der vier Modelle unterscheiden sich stark voneinander. Die getunten Modelle haben eine deutlich bessere Accuracy. Die Accuracy der nicht-getunten und der getunten Modelle haben allerdings eine stark ähnliche Accuracy.

### Salary

**(d)** Berichte den eingestellten Wert von `mtry` für den Random Forest.
4

**(e)** Erzeuge einen Plot, der die Tuningergebnisse für das Tuning des boosted tree model anzeigt.
```{r}
plot(tuned_boosted_tree)
```


**(f)** Erzeuge einen Plot der Variable Importance für den tuned Random Forest.
```{r}
plot(varImp(tuned_rf))
```


**(g)** Erzeuge einen Plot der Variable Importance für das tuned Boosted Tree Model.
```{r}
plot(varImp(tuned_boosted_tree))
```


**(h)** Nach dem Random Forest, was sind die drei wichtigsten Prädiktoren?
```{r}
names(importance(tuned_rf$finalModel)[order(importance(tuned_rf$finalModel), decreasing = TRUE), ][1:3])
```


**(i)** Nach dem Boosted Model, was sind die drei wichtigsten Prädiktoren?
```{r}
rownames(varImp(tuned_boosted_tree)$importance)[order(varImp(tuned_boosted_tree)$importance$Overall, decreasing = TRUE)][1:3]
```


### Transformation

**(j)** Basierend auf diesen Ergebnissen, glauben Sie, dass die Transformation notwendig war?
Ja, die RMSE Ergebnisse zeigen, dass sich die Transformation sehr gelohnt hat. Der RMSE des transformierten Modells ist deutlich niedriger.
